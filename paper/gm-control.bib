@INPROCEEDINGS{abbeel04,
    author = {Pieter Abbeel and Andrew Y. Ng},
    title = {Apprenticeship Learning via Inverse Reinforcement Learning},
    booktitle = {In Proceedings of the Twenty-first International Conference on Machine Learning},
    year = {2004},
    publisher = {ACM Press}
}

@INPROCEEDINGS{ng00,
    author = {Andrew Y. Ng and Stuart Russell},
    title = {Algorithms for Inverse Reinforcement Learning},
    booktitle = {in Proc. 17th International Conf. on Machine Learning},
    year = {2000},
    pages = {663--670},
    publisher = {Morgan Kaufmann}
}

@InProceedings{whitehead91,
  author =	"S. D. Whitehead",
  title =	"A Complexity Analysis of Cooperative Mechanisms in
		 Reinforcement Learning",
  booktitle =	"AAAI-91",
  year = 	"1991",
  pages =	"607--613",
  ref =  	"QQ60",
}

@Article{price01,
  author =	"Bob Price and Craig Boutilier",
  title =	"Imitation and Reinforcement Learning in Agents with
		 Heterogeneous Actions",
  journal =	"Lecture Notes in Computer Science",
  volume =	"2056",
  pages =	"111--??",
  year = 	"2001",
  CODEN =	"LNCSD9",
  ISSN = 	"0302-9743",
  bibdate =	"Sat Feb 2 13:04:12 MST 2002",
  bibsource =	"http://link.springer-ny.com/link/service/series/0558/tocs/t2056.htm",
  URL =  	"http://link.springer-ny.com/link/service/series/0558/bibs/2056/20560111.htm;
		 http://link.springer-ny.com/link/service/series/0558/papers/2056/20560111.pdf",
  acknowledgement = "Nelson H. F. Beebe, Center for Scientific
		 Computing, University of Utah, Department of
		 Mathematics, 110 LCB, 155 S 1400 E RM 233, Salt Lake
		 City, UT 84112-0090, USA, Tel: +1 801 581 5254, FAX: +1
		 801 581 4148, e-mail: \path|beebe@math.utah.edu|,
		 \path|beebe@acm.org|, \path|beebe@computer.org|,
		 \path|beebe@ieee.org| (Internet), URL:
		 \path|http://www.math.utah.edu/~beebe/|",
}

@Misc{konidaris06,
  title =	"Autonomous Shaping: Knowledge Transfer in
		 Reinforcement Learning",
  author =	"George Konidaris",
  publisher =	"ScholarWorks@UMass Amherst",
  year = 	"2006",
  month =	jan # "~01",
  abstract =	"We introduce the use of learned shaping rewards in
		 reinforcement learning tasks, where an agent uses prior
		 experience on a sequence of tasks to learn a portable
		 predictor that estimates intermediate rewards,
		 resulting in accelerated learning in later tasks that
		 are related but distinct. Such agents can be trained on
		 a sequence of relatively easy tasks in order to develop
		 a more informative measure of reward that can be
		 transferred to improve performance on more difcult
		 tasks without requiring a hand coded shaping function.
		 We use a rod positioning task to show that this
		 significantly improves performance even after a very
		 brief training period.",
  bibsource =	"OAI-PMH server at scholarworks.umass.edu",
  oai =  	"oai:scholarworks.umass.edu:cs_faculty_pubs-1098",
  source =	"Computer Science Department Faculty Publication
		 Series",
  subject =	"Computer Sciences",
  URL =  	"http://scholarworks.umass.edu/cs_faculty_pubs/99",
}

@InProceedings{ng99,
  author =	"Andrew Y. Ng and Daishi Harada and Stuart Russell",
  title =	"Policy invariance under reward transformations: theory
		 and application to reward shaping",
  booktitle =	"Proc. 16th International Conf. on Machine Learning",
  publisher =	"Morgan Kaufmann, San Francisco, CA",
  year = 	"1999",
  pages =	"278--287",
}

@Article{wainwright08,
  title =	"Graphical Models, Exponential Families, and
		 Variational Inference",
  author =	"Martin J. Wainwright and Michael I. Jordan",
  journal =	"Foundations and Trends in Machine Learning",
  year = 	"2008",
  number =	"1-2",
  volume =	"1",
  bibdate =	"2009-01-13",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/ftml/ftml1.html#WainwrightJ08",
  pages =	"1--305",
  URL =  	"http://dx.doi.org/10.1561/2200000001",
}

@Article{besag75,
  author =	"J. Besag",
  title =	"Statistical analysis of non-lattice data",
  journal =	"The statistician",
  year = 	"1975",
  number =	"24",
  pages =	"179--195",
}

@INPROCEEDINGS{bergstra10,
     author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
      month = jun,
      title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
       year = {2010},
   location = {Austin, TX},
       note = {Oral Presentation},
        url = {http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf}
}

@Article{hestenes52,
  author =	"M. R. Hestenes and E. Stiefel",
  title =	"Methods of Conjugate Gradients for Solving Linear
		 Systems",
  journal =	"Journal of Research of the National Bureau of
		 Standards",
  year = 	"1952",
  volume =	"49",
  number =	"6",
  pages =	"409--436",
  keywords =	"conjugate gradient alogorithm",
}

@Article{hinton02,
  author =	"Geoffrey E. Hinton",
  title =	"Training Products of Experts by Minimizing Contrastive
		 Divergence",
  journal =	"Neural Computation",
  volume =	"14",
  number =	"8",
  year = 	"2002",
  pages =	"1771--1800",
}

@Misc{rawlik10,
  title =	"Approximate Inference and Stochastic Optimal Control",
  author =	"Konrad Rawlik and Marc Toussaint and Sethu
		 Vijayakumar",
  year = 	"2010",
  month =	sep # "~20",
  abstract =	"We propose a novel reformulation of the stochastic
		 optimal control problem as an approximate inference
		 problem, demonstrating, that such a interpretation
		 leads to new practical methods for the original
		 problem. In particular we characterise a novel class of
		 iterative solutions to the stochastic optimal control
		 problem based on a natural relaxation of the exact dual
		 formulation. These theoretical insights are applied to
		 the Reinforcement Learning problem where they lead to
		 new model free, off policy methods for discrete and
		 continuous problems.",
  bibsource =	"OAI-PMH server at export.arxiv.org",
  oai =  	"oai:arXiv.org:1009.3958",
  subject =	"Computer Science - Learning; Statistics - Machine
		 Learning",
  URL =  	"http://arxiv.org/abs/1009.3958",
}

@Misc{tousaint06,
  title =	"Probabilistic inference for solving ({PO}){MDP}s",
  author =	"Marc Tousaint and Stefan Harmeling and Amos Storkey",
  publisher =	"University of Edinburgh",
  year = 	"2006",
  month =	dec,
  abstract =	"The development of probabilistic inference techniques
		 has made considerable progress in recent years, in
		 particular with respect to exploiting the structure
		 (e.g., factored, hierarchical or relational) of
		 discrete and continuous problem domains. We show that
		 these techniques can be used also for solving Markov
		 Decision Processes (MDPs) or partial observable MDPs
		 (POMDPs) when formulated in terms of a structured
		 dynamic Bayesian network (DBN). The approach is based
		 on an equivalence between maximization of the expected
		 future return in the time-unlimited MDP and likelihood
		 maximization in a related mixture of finite-time MDPs.
		 This allows us to use expectation maximization (EM) for
		 computing optimal policies, using arbitrary inference
		 techniques in the E-step. Unlike previous approaches we
		 can show that this actually optimizes the discounted
		 expected future return for arbitrary reward functions
		 and without assuming an ad hoc finite total time. We
		 first develop the approach for standardMDPs and
		 demonstrate it using exact inference on a discrete maze
		 and Gaussian belief state propagation in non-linear
		 stochastic optimal control problems. Then we present an
		 extension for solving POMDPs. We consider an agent
		 model that includes an internal memory variable used
		 for gating reactive behaviors. Using exact inference on
		 the respective DBN, the EM-algorithm solves complex
		 maze problems by learning appropriate internal memory
		 representations.",
  bibsource =	"OAI-PMH server at eprints.pascal-network.org",
  oai =  	"oai:eprints.pascal-network.org:3389",
  subject =	"Computational, Information-Theoretic Learning with
		 Statistics; Learning/Statistics \& Optimisation; Theory
		 \& Algorithms",
  type = 	"Monograph; NonPeerReviewed",
  URL =  	"http://eprints.pascal-network.org/archive/00003389/;
		 http://eprints.pascal-network.org/archive/00003389/01/ToussaintetAl2006ProbabilisticInferenceForSolvingPOMDPs.pdf",
}