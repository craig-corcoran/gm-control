@INPROCEEDINGS{abbeel04,
    author = {Pieter Abbeel and Andrew Y. Ng},
    title = {Apprenticeship Learning via Inverse Reinforcement Learning},
    booktitle = {In Proceedings of the Twenty-first International Conference on Machine Learning},
    year = {2004},
    publisher = {ACM Press}
}

@INPROCEEDINGS{ng00,
    author = {Andrew Y. Ng and Stuart Russell},
    title = {Algorithms for Inverse Reinforcement Learning},
    booktitle = {in Proc. 17th International Conf. on Machine Learning},
    year = {2000},
    pages = {663--670},
    publisher = {Morgan Kaufmann}
}

@InProceedings{whitehead91,
  author =	"S. D. Whitehead",
  title =	"A Complexity Analysis of Cooperative Mechanisms in
		 Reinforcement Learning",
  booktitle =	"AAAI-91",
  year = 	"1991",
  pages =	"607--613",
  ref =  	"QQ60",
}

@Article{price01,
  author =	"Bob Price and Craig Boutilier",
  title =	"Imitation and Reinforcement Learning in Agents with
		 Heterogeneous Actions",
  journal =	"Lecture Notes in Computer Science",
  volume =	"2056",
  pages =	"111--??",
  year = 	"2001",
  CODEN =	"LNCSD9",
  ISSN = 	"0302-9743",
  bibdate =	"Sat Feb 2 13:04:12 MST 2002",
  bibsource =	"http://link.springer-ny.com/link/service/series/0558/tocs/t2056.htm",
  URL =  	"http://link.springer-ny.com/link/service/series/0558/bibs/2056/20560111.htm;
		 http://link.springer-ny.com/link/service/series/0558/papers/2056/20560111.pdf",
  acknowledgement = "Nelson H. F. Beebe, Center for Scientific
		 Computing, University of Utah, Department of
		 Mathematics, 110 LCB, 155 S 1400 E RM 233, Salt Lake
		 City, UT 84112-0090, USA, Tel: +1 801 581 5254, FAX: +1
		 801 581 4148, e-mail: \path|beebe@math.utah.edu|,
		 \path|beebe@acm.org|, \path|beebe@computer.org|,
		 \path|beebe@ieee.org| (Internet), URL:
		 \path|http://www.math.utah.edu/~beebe/|",
}

@Misc{konidaris06,
  title =	"Autonomous Shaping: Knowledge Transfer in
		 Reinforcement Learning",
  author =	"George Konidaris",
  publisher =	"ScholarWorks@UMass Amherst",
  year = 	"2006",
  month =	jan # "~01",
  abstract =	"We introduce the use of learned shaping rewards in
		 reinforcement learning tasks, where an agent uses prior
		 experience on a sequence of tasks to learn a portable
		 predictor that estimates intermediate rewards,
		 resulting in accelerated learning in later tasks that
		 are related but distinct. Such agents can be trained on
		 a sequence of relatively easy tasks in order to develop
		 a more informative measure of reward that can be
		 transferred to improve performance on more difcult
		 tasks without requiring a hand coded shaping function.
		 We use a rod positioning task to show that this
		 significantly improves performance even after a very
		 brief training period.",
  bibsource =	"OAI-PMH server at scholarworks.umass.edu",
  oai =  	"oai:scholarworks.umass.edu:cs_faculty_pubs-1098",
  source =	"Computer Science Department Faculty Publication
		 Series",
  subject =	"Computer Sciences",
  URL =  	"http://scholarworks.umass.edu/cs_faculty_pubs/99",
}

@InProceedings{ng99,
  author =	"Andrew Y. Ng and Daishi Harada and Stuart Russell",
  title =	"Policy invariance under reward transformations: theory
		 and application to reward shaping",
  booktitle =	"Proc. 16th International Conf. on Machine Learning",
  publisher =	"Morgan Kaufmann, San Francisco, CA",
  year = 	"1999",
  pages =	"278--287",
}

@Article{wainwright08,
  title =	"Graphical Models, Exponential Families, and
		 Variational Inference",
  author =	"Martin J. Wainwright and Michael I. Jordan",
  journal =	"Foundations and Trends in Machine Learning",
  year = 	"2008",
  number =	"1-2",
  volume =	"1",
  bibdate =	"2009-01-13",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/ftml/ftml1.html#WainwrightJ08",
  pages =	"1--305",
  URL =  	"http://dx.doi.org/10.1561/2200000001",
}